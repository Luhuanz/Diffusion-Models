# DDPM

â€‹	diffusion-based generative models were first introduced in 2015 and popularized in 2020 when Ho et al. published the paper â€œDenoising Diffusion Probabilistic Modelsâ€ (DDPMs).

â€‹	In DDPMs, the authors changed the formulation and model training procedures which helped to improve and achieve *â€œimage fidelityâ€*(å›¾åƒçš„ä¿çœŸåº¦) rivaling(åª²ç¾) GANs and established the validity of these new generative algorithms.

The best approach to completely understanding *â€œDenoising Diffusion Probabilistic Modelsâ€* is by å¤ä¹ ç†è®ºï¼ˆ+ ä¸€äº›æ•°å­¦ï¼‰å’Œåº•å±‚ä»£ç  ã€‚

The job of image-based [generative models](https://learnopencv.com/generative-and-discriminative-models/#GenerativeModelling) is to generate **new images that are similar,** in other words, â€œrepresentativeâ€ of our original set of images.(åŸºäºå›¾åƒçš„ç”Ÿæˆæ¨¡å‹çš„å·¥ä½œæ˜¯ç”Ÿæˆç›¸ä¼¼çš„æ–°å›¾åƒï¼Œæ¢å¥è¯è¯´ï¼Œâ€œä»£è¡¨â€ æˆ‘ä»¬çš„åŸå§‹å›¾åƒé›†ã€‚)

We need to create and train generative models because the set of all possible images that can be represented by, say, just (256x256x3) images is enormous. **An image must have the right pixel value combinations to represent something meaningful** (something we can understand).

![An image of a Sunflower.](https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models_sunflower.jpg)

For example, for the above image to represent a â€œSunflowerâ€, the pixels in the image need to be in the right configuration (they need to have the right values). And the space where such images exist is just a fraction of the entire set of images that can be represented by a (256x256x3) image space.  (é™¤äº†imageçš„åƒç´ è¦æ­£ç¡®å¤–ï¼Œè¿™äº›256x256x3ç»„åˆè¿‡çš„å›¾ç‰‡åªæœ‰ä¸€å°éƒ¨åˆ†æ˜¯è¢«è®¤ä¸ºæ˜¯å‡†ç¡®)

The probability distribution function or,more precisely, probability density function (PDF) that captures/models this (data) subspace **remains unknown** and most likely **too complex** to make sense. This is why we need â€˜Generative models â€” To figure out the underlying likelihood function our data satisfies.

Every PDF has a set of parameters that determine the shape and probabilities of the distribution. The shape of the distribution changes as the parameter values change. For example, in the case of a normal distribution, we have mean $\mu$ and variance  $\sigma$ that control the distributionâ€™s center point and spread.

![Effects of changing the parameters of the Gaussian distributions.](https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models_gaussian_distribution_example.png)

## DDPMåŸºæœ¬è¿‡ç¨‹

Diffusion models are a class of generative models inspired by an idea in Non-Equilibrium Statistical Physics(éå¹³è¡¡ç»Ÿè®¡ç‰©ç†å­¦), which states:
***â€œWe can gradually convert one distribution into another using a Markov chainâ€***

* *Deep Unsupervised Learning using Nonequilibrium Thermodynamics, 2015*

Diffusion generative models are composed of two opposite processes: **Forward & Reverse Diffusion Process**.(æ­£å‘æ‰©æ•£å’Œé€†å‘æ‰©æ•£)

### **Forward Diffusion Process**:

***â€œItâ€™s easy to destroy but hard to createâ€***  *â€“ Pearl S. Buck* 

  (æ­£å‘æ˜¯ä¸æ–­çš„åŠ æ­£æ€åˆ†å¸ƒé‡‡æ ·çš„å™ªå£°).

- In the â€œForward Diffusionâ€ process, we **slowly and iteratively(è¿­ä»£) add noise to (corrupt) the images in our training set** such that **they â€œmove out or move awayâ€ from their existing subspace.**
- What we are doing here is converting the unknown and complex distribution that our training set belongs to into one that is easy for us to sample a (data) point from and understand.(æˆ‘ä»¬è¿™é‡Œè¦åšçš„æ˜¯å°†æˆ‘ä»¬çš„trainæ•°æ®æ‰€å±äºçš„å¤æ‚åˆ†å¸ƒè½¬æ¢ä¸ºæ˜“äºæˆ‘ä»¬ä»ä¸­é‡‡æ ·(æ•°æ®)å¹¶ç†è§£çš„åˆ†å¸ƒ)ã€‚
- At the end of the forward process, the images become entirely unrecognizable . The complex data distribution is wholly transformed into a (chosen) simple distribution. Each image gets mapped to a space outside the data subspace.

![Slowly transforming the data distribution in the forward diffusion process as done in diffusion probabilistic models (DDPms).](https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models_forward_process_changing_distribution.png)

### **Reverse Diffusion Process**:

 **By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond.** 

![image-20230402075531534](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230402075531534.png)

1. In the â€œReverse Diffusion process,â€ the idea is to reverse the forward diffusion process.
2. We slowly and iteratively try to reverse the corruption performed on images in the forward process.
3. The reverse process starts where the forward process ends.
4. The benefit of starting from a simple space is that we know how to get/sample a point from this simple distribution (think of it as any point outside the data subspace(çœŸå®çš„å¤æ‚ç©ºé—´çš„ç‚¹)).
5. **And our goal here is to figure out how to return to the data subspace.**
6. However, the problem is that we can take infinite paths starting from a point in this â€œsimpleâ€ space, but only a fraction of them will take us to the â€œdataâ€ subspace. ï¼ˆæˆ‘ä»¬å¯ä»¥ä»è¿™ä¸ªâ€œç®€å•â€ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹å¼€å§‹ï¼Œæœ‰æ— æ•°æ¡è·¯å¾„ï¼Œä½†åªæœ‰ä¸€å°éƒ¨åˆ†ä¼šæŠŠæˆ‘ä»¬å¸¦åˆ°â€œæ•°æ®â€å­ç©ºé—´ï¼‰
7. In diffusion probabilistic models, this is done by referring to the small iterative steps taken during the forward diffusion process. ï¼ˆé€šè¿‡å‚è€ƒå‰å‘æ‰©æ•£è¿‡ç¨‹ä¸­é‡‡å–çš„å°çš„è¿­ä»£æ­¥éª¤æ¥å®ç°ï¼‰
8. The PDF that satisfies the corrupted images **in the forward process** differs slightly at each step.
9. Hence, **in the reverse process,** we use a deep-learning model at each step **to predict the PDF parameters of the forward process.** 
10. **And once we train the modelï¼ˆè®­ç»ƒå®Œæ¨¡å‹ï¼‰, we can start from any point in the simple space and use the model to iteratively take steps to lead us back to the data subspace.** 
11. In reverse diffusion, we iteratively perform the **â€œdenoisingâ€** in small steps, starting from a noisy image. ï¼ˆå°æ­¥å­å»å™ªï¼ŒæŠŠå™ªå£°ç©ºé—´å¸¦å›å­ç©ºé—´ï¼‰
12. This [approach](https://learnopencv.com/wp-content/uploads/2023/01/diffusion-models-unconditional_image_generation-1.gif) for training and generating new samples is much **more stable than GANs** and **better than** previous approaches like variational autoencoders (**VAE**) and **normalizing flows.** 

 Since their introduction in 2020, DDPMs has been the foundation for cutting-edge image generation systems, including DALL-E 2, Imagen, Stable Diffusion, and Midjourney.

##     DPPM å…¬å¼æµç¨‹

![Illustration of the forward and reverse diffusion process in denoising diffusion probabilistic models (DDPMs).](https://learnopencv.com/wp-content/uploads/2023/01/diffusion-models-forwardbackward_process_ddpm.png)

There are two terms mentioned on the arrows:

 $q\left(x_t \mid x_{t-1}\right)$

- This term is also known as the **forward diffusion kernel (FDK).**

- It defines the PDF of an image at timestep$t$ in the forward diffusion process $x_{t}$  given $x_{t-1}$.

- It denotes the â€œtransition functionâ€ applied at each step in the **forward diffusion process**.

 $p_\theta\left(x_{t-1} \mid x_t\right)$

- Similar to the forward process, it is known as the **reverse diffusion kernel (RDK).**
- It stands for the PDF of $x_{t-1}$ given $x_i$ as  parameterized by $\theta$ . The $\theta$ means that the parameters of the distribution of the reverse process are learned using a neural network.
-  Itâ€™s the â€œtransition functionâ€(è¿‡æ¸¡å‡½æ•°) applied at each step in the **reverse diffusion process**. 

##  å‰å‘çš„æ•°å­¦è¿‡ç¨‹

![A modified image of diffusion process illustration focusing on forward diffusion process.](https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models-overall_forward_diffusion_process-1.png)

The distribution $q$   in the forward diffusion process is defined as *Markov Chain* given by:
$$
\begin{aligned}
q\left(x_1, \ldots, x_T \mid x_0\right) & :=\prod_{t=1}^T q\left(x_t \mid x_{t-1}\right) \\
q\left(x_t \mid x_{t-1}\right) & :=\mathcal{N}\left(x_t ; \sqrt{1-\beta_t} x_{t-1}, \beta_t I\right)
\end{aligned}
$$

1. We begin by taking an image from our dataset:  $x_0$.  Mathematically itâ€™s stated as sampling a data point from **the original (but unknown) data distribution**(å¤æ‚åˆ†å¸ƒ):  $x_0 \sim q\left(x_0\right)$
2.  **The PDF of the forward process** is the product of individual distribution(å•ä¸ªåˆ†å¸ƒçš„ä¹˜ç§¯) starting from timestep  $1 \rightarrow T$.
3.  The forward diffusion process is fixed and known.
4. All the intermediate noisy images starting from timestep **1** to **T** are also called â€œlatents.â€ ï¼ˆä¸­é—´å™ªå£°å›¾åƒä¹Ÿè¢«ç§°ä¸º "æ½œåœ¨"ï¼‰The dimension of the latents is the same as the original image.
5. The PDF used to define the FDK is a â€œ**Normal/Gaussian distributionâ€** .
6. At each timestep $t$, the parameters that define the distribution of image $x_t$ are set as:
   - Mean: $\sqrt{1-\beta_t} x_{t-1}$
   - Covariance: $\beta_t I$
7. The term $\beta$ (beta) is known as the **"diffusion rate"** and is precalculated(é¢„å…ˆ)using a â€œvariance schedulerâ€. ï¼ˆ$\beta$ èƒ½è¢«é¢„å…ˆè®¡ç®—å‡ºæ¥ï¼‰ The term $I$ is an identity matrix.  Therefore, the distribution at each time step is called **Isotropic Gaussian.** (å„å‘åŒæ€§çš„é«˜æ–¯åˆ†å¸ƒï¼ˆ**çƒå½¢é«˜æ–¯åˆ†å¸ƒ**ï¼‰æŒ‡çš„æ˜¯å„ä¸ªæ–¹å‘æ–¹å·®éƒ½ä¸€æ ·çš„å¤šç»´é«˜æ–¯åˆ†å¸ƒï¼Œåæ–¹å·®ä¸ºæ­£å®æ•°ä¸ identity matrix ç›¸ä¹˜)
8. The original image is corrupted at each time step by adding a small amount of **gaussian noise**$$(\varepsilon)$$(æ ‡å‡†é«˜æ–¯åˆ†å¸ƒ) .The amount of noise added is regulated by the scheduler.(æ·»åŠ çš„å™ªå£°é‡ç”±scheduleræ§åˆ¶ -> $\beta$ç”±variance schedulerå†³å®š)
9. By choosing **sufficiently large timesteps** and defining **a well-behaved schedule of** $\beta_t$ *the repeated application of FDK gradually converts the data distribution to be nearly an isotropic gaussian distribution.* (æœ€åè¢«ç ´åçš„å›¾åƒåˆ†å¸ƒæ¥è¿‘ isg åˆ†å¸ƒ)

###### å¦‚ä½•ä»$x_{t-1}$å¾—åˆ°å›¾åƒ$x_t$ï¼Œä»¥åŠå¦‚ä½•åœ¨æ¯ä¸ªæ—¶é—´æ­¥éª¤ä¸­åŠ å…¥å™ªå£° (å…³é”®)

we can easily sample image $x_t$ from a normal distribution as:
$$
\begin{aligned}
& x_t=\sqrt{1-\beta_t} x_{t-1}+\sqrt{\beta_t} \epsilon  \\
& \text {; where } \epsilon \sim \mathcal{N}(0, I) \\
&
\end{aligned}
$$

1. the epsilon $\varepsilon$ is the "**noise**" term that is randomly sampled from the standard gaussian distribution and is first scaled and then added (scaled)  $x_{t-1}$ ( Îµ æ˜¯ä»æ ‡å‡†é«˜æ–¯åˆ†å¸ƒä¸­éšæœºé‡‡æ ·çš„â€œå™ªå£°â€é¡¹ï¼Œé¦–å…ˆè¢«ç¼©æ”¾ï¼Œç„¶åæ·»åŠ ï¼ˆç¼©æ”¾)$x_{t-1}$ é¡¹ã€‚
2. In this way, starting from $x_0$, the original image is iteratively corrupted from  $t=1 \ldots T$

###### *linear variance scheduler*

 In practice, the authors of DDPMs use a â€œ*linear variance schedulerâ€* and define $\beta$ in range **[0.0001,0.02]** and set total timesteps $T=1000$

**Diffusion models scale down the data with each forward process step **(æŒ‰æ¯”ä¾‹ç¼©å°æ•°æ®) **(by a   $\sqrt{1-\beta_t}$  factor)**  **so that variance does not grow when adding noise**. (åœ¨æ·»åŠ å™ªå£°æ—¶æ–¹å·®ä¸ä¼šå¢åŠ ) 

$\beta$ å¯ä»¥æå‰è¢«è®¡ç®— ä¸‹å›¾

![A graph to show how the value of beta terms changes depending on the timesteps.](https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models_forward_diffusion_variance_scheduler.png)

â€‹     **Thereâ€™s a problem here, which results in an inefficient forward process**                                                           

Whenever we need a latent sample $x$ at timestep $t$, we have to perform $t-1$  steps ï¼ˆt=1$\dots$t-1ï¼‰in the Markov chain.

![In the current formulation of forward diffusion kernel we have no choice but to traverse the Markov chain to get to timestep t.](https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models_forward_diffusion_problem-1-1024x144.png)

To fix this, the authors of the DDPM reformulated the kernel to directly go from timestep 0 (i.e., from the original image) to timestep $t$ in the process. (ä» t=0 ç›´æ¥åˆ° t)

**To do so, two additional terms are defined: **
$$
\begin{aligned}
\alpha_t & :=1-\beta_t \\
\bar{\alpha}_t & :=\prod_{s=1}^t \alpha_s
\end{aligned}
$$
where $\bar{\alpha}_t$ is  a cumulative product(ç´¯ä¹˜) of $\alpha$ from 1 to $t$.

ç”¨é«˜æ–¯åˆ†å¸ƒçš„åŠ æ³•ç‰¹æ€§å°†$\beta$ ç”¨$\bar{\alpha}$ æ›¿æ¢æ‰ ï¼ˆå…·ä½“æ¨å¯¼ï¼Ÿï¼‰
$$
\begin{aligned}
q\left(x_t \mid x_0\right) & :=\mathcal{N}\left(x_t ; \sqrt{\bar{\alpha}_t} x_{t-1},\left(1-\bar{\alpha}_t\right) I\right) \\
x_t & :=\sqrt{\overline{\alpha_t}} x_0+\sqrt{1-\bar{\alpha}_t} \epsilon
\end{aligned}
$$
ğŸš€ Using the above formulation, we can sample at any arbitrary timestep $t$ in the Markov chain.

æ³¨ ï¼š é«˜æ–¯åˆ†å¸ƒçš„åŠ æ³•ç‰¹æ€§
$$
\begin{aligned}
& X \sim N\left(\mu_X, \sigma_X^2\right) \\
& Y \sim N\left(\mu_Y, \sigma_Y^2\right) \\
& Z=X+Y
\end{aligned}
$$
then
$$
Z \sim N\left(\mu_X+\mu_Y, \sigma_X^2+\sigma_Y^2\right)
$$

## The Reverse Diffusion  æ•°å­¦ç»†èŠ‚

**â€œIn the reverse diffusion process, the task is to learn a finite-time (within T timesteps) reversal of the forward diffusion process.**

This basically means that we have to â€œundoâ€ the forward process , to remove the noise added in the forward process iteratively. It is done using a neural network model.ï¼ˆæˆ‘ä»¬å¿…é¡» â€œæ’¤æ¶ˆâ€ å‰å‘è¿‡ç¨‹ï¼Œå³è¿­ä»£åœ°å»é™¤å‰å‘è¿‡ç¨‹ä¸­æ·»åŠ çš„å™ªå£°ã€‚ å®ƒæ˜¯ä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹å®Œæˆçš„ã€‚ï¼‰

In the forward process, the transitions function $q$ was  defined using **a Gaussian**, so what function should be used for the reverse process $p$ ?  *What should the neural network learn?* 

![A modified illustration of diffusion process focusing on reverse diffusion process.](https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models-overall_reverse_diffusion_process.png)

- In 1949, W. Feller showed that, for gaussian distributions, the diffusion processâ€™s reversal has the same functional form as the forward process.
- This means that similar to the FDK, which is defined as a normal distribution, we can use **the same functional form (a gaussian distribution)** to define the reverse diffusion kernel. 
- The reverse process is also a Markov chain where a **neural network predicts the parameters for the reverse diffusion kernel at each timestep.** 
- During training, the learned  of the parameters (reverse) should be close to the parameters of the FDKâ€™s posterior at each timestep.  
- **We want this because if we follow the forward trajectory in reverse, we may return to the original data distribution.** 
- we would also learn **how to generate new samples that closely match the underlying (ä¸­é—´å±‚)data distribution**, **starting from a pure gaussian noise** (we do not have access to the forward process during inference).  



###### æ•°å­¦ç»†èŠ‚

1.  The Markov chain for the reverse diffusion starts from where the forward process ends,   at timestep **T**, where the data distribution has been converted into (nearly an) **isotropic gaussian distribution.**($X_T$ çš„ä½ç½®)
   $$
   \begin{array}{r}
   q\left(x_T\right) \approx \mathcal{N}\left(x_t ; 0, I\right) \\
   p\left(x_T\right):=\mathcal{N}\left(x_t ; 0, I\right)  
   \end{array}
   $$
    

2. The PDF of the reverse diffusion process is an â€œintegralâ€ over all the possible pathways we can take to arrive at a data sample (in the same distribution as the original) starting from pure noise **$x_T$**. (åå‘æ‰©æ•£è¿‡ç¨‹çš„PDFæ˜¯å¯¹æ‰€æœ‰å¯èƒ½çš„è·¯å¾„çš„ "ç§¯åˆ†"ï¼Œæˆ‘ä»¬å¯ä»¥ä»çº¯å™ªå£°$x_T$å¼€å§‹åˆ°è¾¾ä¸€ä¸ªæ•°æ®æ ·æœ¬ï¼ˆä¸åŸå§‹åˆ†å¸ƒç›¸åŒï¼‰)

$$
\begin{gathered}
p_\theta\left(x_0\right):=\int p_\theta\left(x_{0: T}\right) d x_{1: T} \\
p_\theta\left(\mathbf{x}_{0: T}\right):=p\left(\mathbf{x}_T\right) \prod_{t=1}^T p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right), \quad p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right):=\mathcal{N}\left(\mathbf{x}_{t-1} ; \boldsymbol{\mu}_\theta\left(\mathbf{x}_t, t\right), \mathbf{\Sigma}_\theta\left(\mathbf{x}_t, t\right)\right)
\end{gathered}
$$

![All equations used to define the forward and reverse diffusion process in denoising diffusion probabilistic models (DDPMs). ](https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models-forward_and_backward_equations.png)



# ä¼˜åŒ–ç›®æ ‡ï¼ˆæŸå¤±å‡½æ•°ï¼‰

åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒç›®æ ‡ç›¸å½“äº â€œæœ€å¤§åŒ–ç”Ÿæˆçš„æ ·æœ¬ï¼ˆåœ¨åå‘è¿‡ç¨‹ç»“æŸæ—¶ï¼‰ï¼ˆxï¼‰å±äºåŸå§‹æ•°æ®åˆ†å¸ƒçš„å¯¹æ•°ä¼¼ç„¶â€ã€‚

æˆ‘ä»¬å°†æ‰©æ•£æ¨¡å‹ä¸­çš„transition functions å®šä¹‰ä¸º â€œé«˜æ–¯å‡½æ•°â€ã€‚ ä¸ºäº†æœ€å¤§åŒ–é«˜æ–¯åˆ†å¸ƒçš„å¯¹æ•°ä¼¼ç„¶ï¼Œå°±æ˜¯å°è¯•æ‰¾åˆ°åˆ†å¸ƒ $(\boldsymbol{\mu},\sigma^2)$çš„å‚æ•°ï¼Œä½¿å…¶æœ€å¤§åŒ–å±äºä¸åŸå§‹æ•°æ®åˆ†å¸ƒç›¸åŒçš„ï¼ˆç”Ÿæˆçš„ï¼‰æ•°æ®çš„ â€œä¼¼ç„¶â€ æ•°æ®ã€‚

 ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬å°†æŸå¤±å‡½æ•° (L) å®šä¹‰ä¸ºç›®æ ‡å‡½æ•°çš„è´Ÿå€¼ã€‚ æ‰€ä»¥ $\mathrm{p} \boldsymbol{\theta}(\mathrm{x}_0)$çš„é«˜å€¼æ„å‘³ç€ä½æŸå¤±ï¼Œåä¹‹äº¦ç„¶ ã€‚
$$
\begin{aligned}
p_\theta\left(x_0\right) & :=\int p_\theta\left(x_{0: T}\right) d x_{1: T} \\
L & =-\log \left(p_\theta\left(x_0\right)\right)
\end{aligned}
$$
äº‹å®è¯æ˜ï¼Œè¿™æ˜¯æ£˜æ‰‹çš„ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦åœ¨éå¸¸é«˜ç»´ï¼ˆåƒç´ ï¼‰ç©ºé—´ä¸Šç§¯åˆ†ä»¥è·å¾— T æ—¶é—´æ­¥é•¿çš„è¿ç»­å€¼ã€‚ï¼ˆVAEï¼‰or ELBO.

 ä½œè€…ä» VAE ä¸­æ±²å–çµæ„Ÿï¼Œå¹¶ä½¿ç”¨å˜åˆ†ä¸‹ç•Œ (VLB)ï¼ˆä¹Ÿç§°ä¸º â€œè¯æ®ä¸‹ç•Œâ€(ELBO)ï¼‰é‡æ–°åˆ¶å®šè®­ç»ƒç›®æ ‡ï¼Œè¿™å°±æ˜¯è¿™ä¸ªçœ‹èµ·æ¥å¾ˆå“äººçš„æ–¹ç¨‹ğŸ‘»ï¼š
$$
\mathbb{E}\left[-\log p_\theta\left(\mathbf{x}_0\right)\right] \leq \mathbb{E}_q\left[-\log \frac{p_\theta\left(\mathbf{x}_{0: T}\right)}{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)}\right]=\mathbb{E}_q\left[-\log p\left(\mathbf{x}_T\right)-\sum_{t \geq 1} \log \frac{p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)}{q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right)}\right]=: L
$$
ç»è¿‡ä¸€äº›ç®€åŒ–åï¼ŒDDPM ä½œè€…å¾—å‡ºäº†æœ€ç»ˆçš„ $L_{vlb}$â€”â€” å˜åˆ†ä¸‹ç•ŒæŸå¤±é¡¹ï¼š
$$
\mathbb{E}_q[\underbrace{D_{\mathrm{KL}}\left(q\left(\mathbf{x}_T \mid \mathbf{x}_0\right) \| p\left(\mathbf{x}_T\right)\right)}_{L_T}+\sum_{t>1} \underbrace{D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right) \| p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)\right)}_{L_{t-1}} \underbrace{-\log p_\theta\left(\mathbf{x}_0 \mid \mathbf{x}_1\right)}_{L_0}]
$$
æˆ‘ä»¬å¯ä»¥å°†ä¸Šè¿° $L_{vlb}$ æŸå¤±é¡¹åˆ†è§£ä¸ºå•ç‹¬çš„æ—¶é—´æ­¥é•¿ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
$$
\begin{aligned}
L_{\mathrm{vlb}} & :=L_0+L_1+\ldots+L_{T-1}+L_T \\
L_0 & :=-\log p_\theta\left(x_0 \mid x_1\right) \\
L_{t-1} & :=D_{K L}\left(q\left(x_{t-1} \mid x_t, x_0\right) \| p_\theta\left(x_{t-1} \mid x_t\right)\right) \\
L_T & :=D_{K L}\left(q\left(x_T \mid x_0\right) \| p\left(x_T\right)\right)
\end{aligned}
$$
è¿™ä¸ªæŸå¤±å‡½æ•°å¾ˆå¤§,ä½†æ˜¯ DDPM çš„ä½œè€…é€šè¿‡å¿½ç•¥ä»–ä»¬ç®€åŒ–çš„æŸå¤±å‡½æ•°ä¸­çš„ä¸€äº›é¡¹æ¥è¿›ä¸€æ­¥ç®€åŒ–å®ƒã€‚

->   å»æ‰$L_{0}$ and $L{T}$

1. $L_{0}$ â€“ The authors got better results without this.
2.  $L_T$ â€“   è¿™å°±æ˜¯æ­£å‘è¿‡ç¨‹ä¸­çš„ æœ€åéšå±‚çš„åˆ†å¸ƒå’Œåå‘è¿‡ç¨‹ä¸­çš„ç¬¬ä¸€ä¸ªæ½œåŠ¿åˆ†å¸ƒä¹‹é—´çš„ KLç›¸ä¼¼æ€§ã€‚ç„¶è€Œï¼Œè¿™é‡Œæ²¡æœ‰æ¶‰åŠç¥ç»ç½‘ç»œå‚æ•°ï¼Œæ‰€ä»¥é™¤äº†å®šä¹‰ä¸€ä¸ªå¥½çš„variance schedulerå’Œä½¿ç”¨å¤§çš„large timesteps ï¼Œä½¿å®ƒä»¬éƒ½ä»£è¡¨ä¸€ä¸ªan Isotropic Gaussian distributionï¼Œæˆ‘ä»¬å¯¹æ­¤æ— èƒ½ä¸ºåŠ›ã€‚

So $L_{t-1}$ is the only loss   å®ƒæ˜¯å‰å‘è¿‡ç¨‹ï¼ˆä»¥ $x_t$ å’Œåˆå§‹æ ·æœ¬ $x_0$ ä¸ºæ¡ä»¶ï¼‰çš„ â€œåéªŒâ€ ä¸å‚æ•°åŒ–åå‘æ‰©æ•£è¿‡ç¨‹ä¹‹é—´çš„ KL æ•£åº¦ã€‚ 
$$
L_{v l b}:=L_{t-1}:=D_{K L}\left(q\left(x_{t-1} \mid x_t, x_0\right) \| p_\theta\left(x_{t-1} \mid x_t\right)\right)
$$
The term  $q\left(x_{t-1} \mid x_t, x_0\right)$  is referred to as ***â€œforward process posterior distribution.â€*** 

åœ¨è®­ç»ƒæœŸé—´çš„å·¥ä½œæ˜¯è¿‘ä¼¼ / ä¼°è®¡æ­¤ï¼ˆé«˜æ–¯ï¼‰åéªŒçš„å‚æ•°ï¼Œä»¥ä½¿ KL æ•£åº¦å°½å¯èƒ½å°ã€‚

![Image to illustrate the point why we need to minimize the KL divergence between the forward posterior and the reverse process in denoising diffusion probabilistic models (DDPMs). ](https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models-forward_posterior_reverse_equations.png)

The parameters of the posterior distribution are as follows:
$$
q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ; \tilde{\boldsymbol{\mu}}_t\left(\mathbf{x}_t, \mathbf{x}_0\right), \tilde{\beta}_t \mathbf{I}\right),
$$
$$
\text { where } \quad \tilde{\boldsymbol{\mu}}_t\left(\mathbf{x}_t, \mathbf{x}_0\right):=\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \mathbf{x}_0+\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \mathbf{x}_t \quad \text { and } \quad \tilde{\beta}_t:=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t
$$

To further simplify the task of the model, **the authors decided to fix the variance to a constant ** $\beta_t$

æ¨¡å‹åªéœ€è¦å­¦ä¹ é¢„æµ‹ä¸Šé¢çš„æ–¹ç¨‹ã€‚ å¹¶ä¸”åå‘æ‰©æ•£å†…æ ¸è¢«ä¿®æ”¹ä¸ºï¼š
$$
\begin{gathered}
p_\theta\left(x_{t-1} \mid x_t\right)=\mathcal{N}\left(x_{t-1} ; \mu_\theta\left(x_t, t\right), \Sigma_\theta\left(x_t, t\right)\right) \\
\downarrow \\
p_\theta\left(x_{t-1} \mid x_t\right)=\mathcal{N}\left(x_{t-1} ; \mu_\theta\left(x_t, t\right), \sigma^2 I\right)
\end{gathered}
$$
ç”±äºæˆ‘ä»¬ä¿æŒæ–¹å·®ä¸å˜ï¼Œæœ€å°åŒ– KL æ•£åº¦å°±åƒæœ€å°åŒ–ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒ q å’Œ p çš„å‡å€¼ (ğµ) ä¹‹é—´çš„å·®å¼‚ï¼ˆæˆ–è·ç¦»ï¼‰ä¸€æ ·ç®€å•:
$$
L_{t-1}=\mathbb{E}_q\left[\frac{1}{2 \sigma_t^2}\left\|\tilde{\boldsymbol{\mu}}_t\left(\mathbf{x}_t, \mathbf{x}_0\right)-\boldsymbol{\mu}_\theta\left(\mathbf{x}_t, t\right)\right\|^2\right]+C
$$